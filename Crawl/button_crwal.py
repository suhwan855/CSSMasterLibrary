# mini_crawl_uiverse_buttons_to_db.py
# Author DOM ì¶”ì¶œ + tqdm ì§„í–‰ë¥  + logging ìš”ì•½ ì¶œë ¥ + DB insert (ê·¼ë³¸ í•´ê²° ì ìš©)
# pip install tqdm selenium webdriver-manager psycopg[binary]

import re
import time
import html as htmlmod
import logging
from typing import List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from tqdm import tqdm
import psycopg
from psycopg.rows import dict_row

from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# ===================== ë¡œê¹… ì„¤ì • =====================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger("uiverse")

# ===================== ì„¤ì • =====================
BASE = "https://uiverse.io"
CATEGORY = "Buttons"  # ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œ ë°”ê¿”ì£¼ì„¸ìš”
LIST_URL = BASE + "/buttons?page={page}"

HEADLESS_LIST   = False   # ëª©ë¡(ìœˆë„ìš° ëœ¸)
HEADLESS_DETAIL = True    # ìƒì„¸(ìœˆë„ìš° ì•ˆ ëœ¸)

MAX_WORKERS   = 6
SCROLL_STEPS  = 18
SCROLL_DY     = 1800
SCROLL_PAUSE  = 0.35

# /Author/slug-123 (ëŒ€ë¶€ë¶„ Uiverse ìƒì„¸ URL íŒ¨í„´)
PAT = re.compile(r"^/[A-Za-z0-9-]+/[A-Za-z0-9-]+-\d+$")

# ---- DB ì—°ê²° ì„¤ì • (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •) ----
PG_DSN = {
    "host": "220.74.18.216",
    "port": 5432,
    "user": "admin",
    "password": "qwe123",
    "dbname": "daelim",
}
TABLE = "components_tbl_test"

# ===================== ìœ í‹¸ =====================
def make_driver(headless: bool = True):
    opt = webdriver.ChromeOptions()
    if headless:
        opt.add_argument("--headless=new")
    opt.add_argument("--window-size=1366,3000")
    opt.add_argument("--lang=ko-KR")
    opt.add_argument("--disable-gpu")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-dev-shm-usage")
    opt.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    )
    opt.add_experimental_option("excludeSwitches", ["enable-automation"])
    opt.add_experimental_option("useAutomationExtension", False)
    opt.add_argument("--disable-blink-features=AutomationControlled")

    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=opt)
    driver.set_page_load_timeout(60)
    driver.implicitly_wait(2)
    try:
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined});"
        })
    except Exception:
        pass
    return driver

def slug(url: str) -> str:
    return re.sub(r"[^A-Za-z0-9_-]+", "-", url.strip("/").split("/")[-1]).strip("-").lower() or "component"

def humanize_slug(s: str) -> str:
    s = re.sub(r"-\d+$", "", s)  # ë’¤ìª½ ìˆ«ì ì œê±°
    parts = [p for p in s.replace("_", "-").split("-") if p]
    return " ".join(w.capitalize() for w in parts) or "Component"

def parse_author_from_url(detail_url: str) -> Optional[str]:
    # https://uiverse.io/<author>/<slug-123>
    try:
        path = detail_url.split("://", 1)[-1].split("/", 1)[-1]
        bits = [b for b in path.split("/") if b]
        return bits[0] if len(bits) >= 2 else None
    except Exception:
        return None

# ========= (A) ê·¼ë³¸ í•´ê²°ìš© ì •ë¦¬ ìœ í‹¸: CSS-only íŒë³„ & ì—”í‹°í‹°/ì´ìŠ¤ì¼€ì´í”„ ì •ë¦¬ =========
CSS_ONLY_RE = re.compile(r"^[^{]*\{[\s\S]*\}[\s\S]*$", re.M)

def _looks_css_only(s: str) -> bool:
    if not s:
        return False
    t = s.strip()
    # íƒœê·¸ ê¸°í˜¸ê°€ ìˆìœ¼ë©´ HTMLì¼ ê°€ëŠ¥ì„±ì´ í¼
    if "<" in t and ">" in t:
        return False
    # ì¤‘ê´„í˜¸ ë¸”ë¡ì´ ë³´ì´ë©´ CSS ê°€ëŠ¥ì„±
    return bool(CSS_ONLY_RE.search(t))

def _clean_piece(s: str) -> str:
    """ì—”í‹°í‹° ë³µì› + í”í•œ ì´ìŠ¤ì¼€ì´í”„ ì œê±° + ì•ë’¤ ê³µë°± ì œê±°"""
    if not s:
        return ""
    s = htmlmod.unescape(s)
    s = s.replace("&lt;", "<").replace("&gt;", ">").replace("&amp;", "&")
    return s.strip()

# ===================== ëª©ë¡: ë§í¬ ìˆ˜ì§‘ =====================
def collect_links(page_num: int) -> List[str]:
    d = make_driver(headless=HEADLESS_LIST)
    try:
        d.get(LIST_URL.format(page=page_num))

        # ë©”ì¸ ë„ì°© ëŒ€ê¸°
        try:
            WebDriverWait(d, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, "main")))
        except TimeoutException:
            pass

        # "Get code" ë˜ëŠ” data-discover ëŒ€ê¸°
        try:
            WebDriverWait(d, 10).until(
                EC.presence_of_element_located((
                    By.XPATH,
                    "//*[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'get code')]"
                ))
            )
        except TimeoutException:
            try:
                WebDriverWait(d, 6).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "a[data-discover='true'][href^='/']"))
                )
            except TimeoutException:
                pass

        # ìŠ¤í¬ë¡¤ë¡œ ì§€ì—° ë¡œë”© ìš”ì†Œ ë¡œë“œ
        for _ in range(SCROLL_STEPS):
            d.execute_script(f"window.scrollBy(0, {SCROLL_DY});")
            time.sleep(SCROLL_PAUSE)
        d.execute_script("window.scrollTo(0, 0);")

        links = set()

        # 1) "Get code" ì•µì»¤
        for a in d.find_elements(
            By.XPATH,
            "//a[contains(translate(normalize-space(.),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'get code') and starts-with(@href,'/')]"
        ):
            href = a.get_attribute("href") or ""
            rel = "/" + href.split("/", 3)[-1] if href.startswith("http") else href
            if PAT.match(rel):
                links.add(BASE + rel)

        # 2) backup: data-discover
        for a in d.find_elements(By.CSS_SELECTOR, "a[data-discover='true'][href^='/']"):
            href = a.get_attribute("href") or ""
            rel = "/" + href.split("/", 3)[-1] if href.startswith("http") else href
            if PAT.match(rel):
                links.add(BASE + rel)

        # 3) ìµœí›„: ëª¨ë“  a[href^='/']ì—ì„œ ì •ê·œì‹ í•„í„°
        if not links:
            for a in d.find_elements(By.XPATH, "//a[starts-with(@href,'/')]"):
                href = a.get_attribute("href") or ""
                rel = "/" + href.split("/", 3)[-1] if href.startswith("http") else href
                if PAT.match(rel):
                    links.add(BASE + rel)

        return sorted(links)
    finally:
        d.quit()

# ===================== ìƒì„¸: íƒ­ í™œì„±í™” â†’ íŒ¨ë„ ë²”ìœ„ì—ì„œ ì½”ë“œ ì½ê¸° =====================
def _click_tab_and_get_panel(d, tab_key: str, wait: WebDriverWait):
    """tab_key: 'html'|'css' â†’ role=tab ë²„íŠ¼ í´ë¦­ í›„ aria-controls íŒ¨ë„ ë°˜í™˜"""
    btn = None
    try:
        btn = d.find_element(By.CSS_SELECTOR, f"button[role='tab'][id*='trigger-{tab_key}']")
    except Exception:
        label = tab_key.upper()
        for xp in (
            f"//button[@role='tab' and normalize-space()='{label}']",
            f"//button[@role='tab' and contains(.,'{label}')]",
            f"//*[self::a or self::button][@role='tab' and contains(.,'{label}')]",
        ):
            try:
                btn = d.find_element(By.XPATH, xp); break
            except Exception:
                pass
    if not btn:
        return None

    # í™œì„±í™” ì‹œë„
    try:
        d.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
        btn.click(); time.sleep(0.15)
        for _ in range(3):
            if (btn.get_attribute("data-state") or "").lower() == "active":
                break
            btn.click(); time.sleep(0.15)
    except Exception:
        pass

    panel = None
    try:
        panel_id = btn.get_attribute("aria-controls") or ""
        if panel_id:
            wait.until(EC.presence_of_element_located((By.ID, panel_id)))
            panel = d.find_element(By.ID, panel_id)
            d.execute_script("arguments[0].scrollIntoView({block:'center'});", panel)
            time.sleep(0.1)
    except Exception:
        panel = None
    return panel

def _read_code_from_panel(d, panel, prefer_id: str) -> str:
    """panel ë‚´ë¶€ì—ì„œ textarea.value ìš°ì„ ìœ¼ë¡œ ì½”ë“œ ì½ê¸°"""
    root = panel if panel is not None else d

    # 1) idë¡œ ì§ì ‘
    try:
        el = root.find_element(By.ID, prefer_id)
        d.execute_script("arguments[0].scrollIntoView({block:'center'});", el)
        time.sleep(0.05)
        val = (d.execute_script("return arguments[0].value;", el) or "").strip()
        if val:
            return val
    except Exception:
        pass

    # 2) ê°™ì€ í´ë˜ìŠ¤ì˜ textarea
    try:
        areas = root.find_elements(By.CSS_SELECTOR, "textarea.npm__react-simple-code-editor__textarea")
        for t in areas:
            v = (t.get_attribute("value") or "").strip()
            if v:
                return v
    except Exception:
        pass

    # 3) pre > code
    try:
        codes = root.find_elements(By.CSS_SELECTOR, "pre code")
        for c in codes:
            v = (c.text or "").strip()
            if v:
                return v
    except Exception:
        pass

    # 4) contenteditable
    try:
        edits = root.find_elements(By.CSS_SELECTOR, "[contenteditable='true']")
        for e in edits:
            v = (e.get_attribute("textContent") or "").strip()
            if v:
                return v
    except Exception:
        pass

    return ""

def _extract_author_dom_first(d) -> Optional[str]:
    # ì‘ì„±ì DOM ìš°ì„  ì¶”ì¶œ, ì‹¤íŒ¨ ì‹œ None
    try:
        el = d.find_element(By.CSS_SELECTOR, "div.card__nickname")
        tx = (el.text or "").strip().lstrip("@")
        if tx:
            return tx
    except Exception:
        pass
    for sel in [
        "[class*='card__nickname']",
        ".card__nickname.text-color",
        "div.card__nickname.text-color.flex.items-center",
        "[class*='card__nickname'] a",
    ]:
        try:
            el = d.find_element(By.CSS_SELECTOR, sel)
            tx = (el.text or "").strip().lstrip("@")
            if tx:
                return tx
        except Exception:
            pass
    return None

def read_codes(detail_url: str) -> Tuple[str, str, str, str]:
    """
    returns: (slug, html, css, author)
    """
    d = make_driver(headless=HEADLESS_DETAIL)
    try:
        d.get(detail_url)
        wait = WebDriverWait(d, 15)
        time.sleep(0.6)

        # (ìˆìœ¼ë©´) ì¿ í‚¤/ëª¨ë‹¬ ë‹«ê¸° â€” ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
        for xp in (
            "//button[contains(.,'Accept') or contains(.,'I agree') or contains(.,'Got it') or contains(.,'í™•ì¸')]",
            "//div[contains(@class,'cookie') or contains(@class,'modal')]//button",
        ):
            try:
                d.find_element(By.XPATH, xp).click(); time.sleep(0.2)
            except Exception:
                pass

        author = _extract_author_dom_first(d) or None
        if not author:
            author = parse_author_from_url(detail_url) or None

        # HTML / CSS íƒ­ â†’ ê° íŒ¨ë„
        html_panel = _click_tab_and_get_panel(d, "html", wait)
        css_panel  = _click_tab_and_get_panel(d, "css",  wait)

        # ê° íƒ­ íŒ¨ë„ ë‚´ë¶€ì—ì„œ ì½ê¸°
        html_raw = _read_code_from_panel(d, html_panel, "codeArea2")
        css      = _read_code_from_panel(d, css_panel,  "codeArea1")

        # ë°±ì—… ê²½ë¡œ: ì „ì²´ ë¬¸ì„œ ìŠ¤ìº”
        if not (html_raw and css):
            for t in d.find_elements(By.CSS_SELECTOR, "textarea.npm__react-simple-code-editor__textarea"):
                v = (t.get_attribute("value") or "").strip()
                if not v:
                    continue
                vu = htmlmod.unescape(v)
                if (not html_raw) and ("<" in vu and ">" in vu and "{" not in vu[:200]):
                    html_raw = v
                if (not css) and ("{" in v and "}" in v and "</" not in v):
                    css = v
                if html_raw and css:
                    break
            if not (html_raw and css):
                for c in d.find_elements(By.CSS_SELECTOR, "pre code"):
                    v = (c.text or "").strip()
                    if not v:
                        continue
                    vu = htmlmod.unescape(v)
                    if (not html_raw) and ("<" in vu and ">" in vu and "{" not in vu[:200]):
                        html_raw = v
                    if (not css) and ("{" in v and "}" in v and "</" not in v):
                        css = v
                    if html_raw and css:
                        break

        # ========= (A) 1ì°¨ ì •ë¦¬: ì—”í‹°í‹°/ì´ìŠ¤ì¼€ì´í”„ ì •ë¦¬ + CSS-only íŒë³„ ë³´ì • =========
        html = _clean_piece(html_raw or "")
        css  = _clean_piece(css or "")

        # HTML íƒ­ì´ ì‚¬ì‹¤ìƒ CSS í…ìŠ¤íŠ¸ì˜€ë˜ ê²½ìš° â†’ CSSë¡œ í•©ì¹˜ê³  HTML ë¹„ìš°ê¸°
        if _looks_css_only(html):
            css = f"{html}\n{css}".strip()
            html = ""

        return slug(detail_url), html, css, (author or "")
    finally:
        d.quit()

# ===================== í•©ë³¸ ë¹Œë”(ê°•í™”íŒ) =====================
DOCT_RE = re.compile(r"<!doctype", re.I)

def _extract_body_inner(html_or_snippet: str) -> str:
    """
    html_or_snippet ì´ ì™„ì „ ë¬¸ì„œë©´ body.innerHTMLë§Œ, snippetì´ë©´ ê·¸ëŒ€ë¡œ.
    ë‹¨, snippet/ë¬¸ìì—´ì´ CSS-onlyë¡œ ë³´ì´ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜(ë°ëª¨ ë§ˆí¬ì—… ìƒì„± íŠ¸ë¦¬ê±°).
    """
    s = (html_or_snippet or "").strip()
    if not s:
        return ""
    # CSS-onlyì²˜ëŸ¼ ë³´ì´ë©´ bodyì— ë„£ì§€ ì•ŠìŒ
    if _looks_css_only(s):
        return ""
    # ì™„ì „ ë¬¸ì„œë©´ bodyë§Œ ì¶”ì¶œ
    if DOCT_RE.search(s) or ("<html" in s.lower()) or ("<body" in s.lower()):
        m = re.search(r"<body[^>]*>([\s\S]*?)</body>", s, re.I)
        return (m.group(1) if m else "").strip()
    # snippetì€ ê·¸ëŒ€ë¡œ
    return s

def _normalize_whitespace(s: str) -> str:
    return re.sub(r"[ \t]+\n", "\n", (s or "")).strip()

def build_combined_document(inner_html: str, css: str) -> str:
    """
    HTML ìŠ¤ë‹ˆí« + CSSë¥¼ ì•ˆì „í•œ 'ì™„ì „ HTML ë¬¸ì„œ'ë¡œ í•©ë³¸ ìƒì„±
    - HTMLì´ ì™„ì „ ë¬¸ì„œì—¬ë„ bodyë§Œ ì¶”ì¶œí•˜ì—¬ ì¤‘ë³µ head ë°©ì§€
    - CSSë§Œ ë“¤ì–´ì˜¨ íŠ¹ìˆ˜ì¼€ì´ìŠ¤ ë°©ì§€(ë°ëª¨ ë§ˆí¬ì—… ìë™ ì¶”ê°€)
    """
    html_part = _extract_body_inner(htmlmod.unescape(inner_html or ""))
    css_part  = _normalize_whitespace(htmlmod.unescape(css or ""))

    html_part = _normalize_whitespace(html_part)

    # HTMLì´ ë¹„ê³  CSSë§Œ ìˆëŠ” ê²½ìš° â†’ ëŒ€í‘œ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë°ëª¨ ìƒì„±
    if (not html_part) and css_part:
        m = re.search(r"\.([A-Za-z_][\w-]*)\s*{", css_part)
        cls = m.group(1) if m else None
        if cls and re.search(r"btn|button", cls, re.I):
            html_part = f'<button class="{cls}">Button</button>'
        elif cls:
            html_part = f'<div class="{cls}">Preview</div>'
        else:
            html_part = "<button>Button</button>"

    return f"""<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Uiverse Preview</title>
<style>
/* --- Uiverse component CSS --- */
{css_part}
html, body {{ margin:0; padding:16px; }}
</style>
</head>
<body>
{html_part}
</body>
</html>"""

# ===================== DB ì €ì¥ =====================
def insert_component(cur, *, name: str, description: Optional[str], preview_html: Optional[str],
                     combined_code: str, library: str, source_url: str,
                     author: Optional[str], category: str):
    sql = f"""
    INSERT INTO {TABLE}
      (components_name, components_description, components_preview_html,
       components_code, components_library, components_source_url,
       components_author, components_category)
    VALUES (%(name)s, %(desc)s, %(preview)s, %(code)s, %(lib)s, %(src)s, %(author)s, %(cat)s)
    """
    cur.execute(sql, {
        "name": name,
        "desc": description,
        "preview": preview_html,     # ì •ì±…ìƒ NULL ì €ì¥
        "code": combined_code,       # ì™„ì „ HTML í•©ë³¸
        "lib": library,
        "src": source_url,
        "author": author,
        "cat": category,
    })

# ===================== ì‹¤í–‰ ë£¨í”„ (ìˆ˜ì§‘ â†’ íŒŒì‹± â†’ DB) =====================
def crawl_to_db(p_from=1, stop_after_two_empty=True, max_workers=MAX_WORKERS):
    page = p_from
    empty_streak = 0
    total_saved = 0

    with psycopg.connect(**PG_DSN, row_factory=dict_row) as conn:
        conn.autocommit = False

        while True:
            log.info("ğŸŒ page %s", page)
            links = collect_links(page)
            log.info("  links: %d", len(links))

            if not links:
                empty_streak += 1
                if stop_after_two_empty and empty_streak >= 2:
                    break
                page += 1
                continue

            empty_streak = 0
            page_ok = page_skip = page_err = 0

            # ìƒì„¸ íŒŒì‹±ì€ ë³‘ë ¬, DBëŠ” ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ìˆœì°¨ ì»¤ë°‹
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futures = [ex.submit(read_codes, u) for u in links]
                with tqdm(total=len(links), desc=f"page {page} details", unit="item") as pbar:
                    for fut in as_completed(futures):
                        try:
                            slug_, html, css, author = fut.result()
                        except Exception as e:
                            log.error("  âš ï¸ detail error: %s", e)
                            page_err += 1
                            pbar.update(1)
                            continue

                        if not (html or css):
                            log.warning("  âš ï¸ skip(no code): %s", slug_)
                            page_skip += 1
                            pbar.update(1)
                            continue

                        # slugë§Œìœ¼ë¡œëŠ” ì› URLì„ ì •í™•íˆ ì—­ë§¤í•‘í•˜ê¸° ì–´ë ¤ì›€ â†’ best-effort
                        source_url = ""
                        for u in links:
                            if slug(u) == slug_:
                                source_url = u
                                break

                        name = humanize_slug(slug_)
                        library = "universe"   # ê¸°ì¡´ ë°ì´í„°ì™€ í˜¸í™˜ ìœ„í•´ ìœ ì§€
                        category = CATEGORY
                        description = None

                        # â˜… HTML+CSS í•©ë³¸(ì™„ì „ ë¬¸ì„œ) â€” ê°•í™”íŒ ë¹Œë” ì‚¬ìš©
                        combined_doc = build_combined_document(html or "", css or "")

                        try:
                            with conn.cursor() as cur:
                                insert_component(
                                    cur,
                                    name=name,
                                    description=description,
                                    preview_html=None,          # â† NULL
                                    combined_code=combined_doc, # â† í•©ë³¸ì„ codeì—
                                    library=library,
                                    source_url=source_url,
                                    author=(author or None),
                                    category=category,
                                )
                            conn.commit()
                            total_saved += 1
                            page_ok += 1
                            log.info("  âœ… inserted: %s (author: %s)", name, author or "unknown")
                        except Exception as e:
                            conn.rollback()
                            page_err += 1
                            log.error("  âŒ insert failed: %s -> %s", name, e)

                        pbar.update(1)

            log.info("page %s summary -> ok:%d skip:%d err:%d", page, page_ok, page_skip, page_err)
            page += 1

    log.info("ğŸ‰ done. total inserted: %d", total_saved)

# ===================== ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ =====================
if __name__ == "__main__":
    crawl_to_db(p_from=1, max_workers=MAX_WORKERS)
