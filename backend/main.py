# main.py
from typing import List, Optional
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import psycopg2
import os, json, requests

from db import init_pool, close_pool, count_by_category, list_by_category

# ======== âš™ï¸ ENV ë¡œë“œ ========
load_dotenv()

# âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ ì½ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PG_DSN = os.getenv("PG_DSN")  # or ê°œë³„ DB_* ë¡œ ì¡°í•©í•´ë„ ë¨
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:5173")

# âœ… í•„ìˆ˜ ê°’ ê²€ì¦ (ì„œë²„ ì¼œì§ˆ ë•Œ ë°”ë¡œ ì‹¤íŒ¨ì‹œì¼œì„œ ì‹¤ìˆ˜ ë°©ì§€)
assert OPENAI_API_KEY, "OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
assert PG_DSN, "PG_DSN í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."

# ======== ğŸ§  ëª¨ë¸/ì„¸ì…˜ ========
model = SentenceTransformer("jhgan/ko-sroberta-multitask")
chat_sessions = {}

# âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ (í•˜ë“œì½”ë”© ì œê±°)
client = OpenAI(api_key=OPENAI_API_KEY)

# âœ… app ìƒì„±
app = FastAPI(title="CSS Components API (async)")

# ======== ğŸŒ CORS ========
origins = [o.strip() for o in ALLOWED_ORIGINS.split(",") if o.strip()]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins or ["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ======== ğŸ“¦ Response Models ========
class ComponentOut(BaseModel):
    id: int
    name: str
    author: Optional[str] = None
    code: str
    category: Optional[str] = None  # Othersì—ì„œ ì‚¬ìš©


class PaginatedResponse(BaseModel):
    items: List[ComponentOut]
    total: int
    page: int
    page_size: int


# ======== âœ… Lifespan ========
@app.on_event("startup")
async def _startup():
    await init_pool()


@app.on_event("shutdown")
async def _shutdown():
    await close_pool()


@app.get("/health")
async def health():
    return {"ok": True}


# ======== ğŸ“š Components API ========
@app.get("/components", response_model=PaginatedResponse)
async def get_components(
    category: str = Query(..., description="ì˜ˆ: buttons/cards/inputs"),
    page: int = Query(1, ge=1),
    page_size: int = Query(24, ge=1, le=100),
):
    total = await count_by_category(category)
    offset = (page - 1) * page_size
    rows = await list_by_category(category, offset, page_size)
    items = [ComponentOut(**row) for row in rows]
    return PaginatedResponse(items=items, total=total, page=page, page_size=page_size)


# ======== ğŸ’¬ Chat Request ========
class ChatRequest(BaseModel):
    query: str
    session_id: str = "default"


# ======== ğŸ” DB ê²€ìƒ‰ (RAG) ========
def retrieve_similar(query, top_k=3):
    emb = model.encode(query).tolist()
    with psycopg2.connect(PG_DSN) as conn:   # âœ… PG_DSN ì‚¬ìš©
        with conn.cursor() as cur:
            cur.execute("""
                SELECT name, html, css, full_code, author, source_type,
                       1 - (embedding <=> %s::vector) AS similarity
                FROM unified_components_v3
                WHERE embedding IS NOT NULL
                ORDER BY embedding <=> %s::vector
                LIMIT %s;
            """, (emb, emb, top_k))
            rows = cur.fetchall()
    return rows


# ======== ğŸ§  Prompt ========
def build_prompt_with_history(history, query):
    examples = retrieve_similar(query, top_k=3)
    examples_text = "\n\n".join([
        f"[{ex[5]}] ì˜ˆì‹œ (by {ex[4]}):\nHTML:\n{ex[1]}\n\nCSS:\n{ex[2]}"
        for ex in examples if ex[1] and ex[2]
    ])

    prompt = f"""
ë‹¹ì‹ ì€ ì„¸ê³„ì ì¸ ì›¹ ì•„í‹°ìŠ¤íŠ¸ì´ì CSS ë””ìì´ë„ˆì…ë‹ˆë‹¤.
ì´ì „ ëŒ€í™” ê¸°ë¡(ë³„ë„ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ë¨)ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³ ,
ì•„ë˜ ì£¼ì œì™€ ì°¸ê³  ì˜ˆì‹œì— ë§ëŠ” ê°ì„±ì ì´ê³  ì˜ˆìˆ ì ì¸ HTML/CSS í˜ì´ì§€ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ì£¼ì œ: "{query}"

ì°¸ê³  ì˜ˆì‹œ (ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë¨):
{examples_text}

ê·œì¹™:
1. ì™„ì „í•œ HTML5 ë¬¸ì„œ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš” (<!DOCTYPE html>ë¶€í„° </html>ê¹Œì§€ í¬í•¨).
2. ì˜¤ì§ HTMLê³¼ ë‚´ë¶€ <style>ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. (JS, í”„ë ˆì„ì›Œí¬, ì™¸ë¶€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ê¸ˆì§€)
3. ìƒ‰ê°ì€ ì¡°í™”ë¡­ê³ , ë””ìì¸ì€ ë¶€ë“œëŸ½ê³  ì„¸ë ¨ë˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.
4. ì—¬ë°±, ë¹„ìœ¨, íƒ€ì´í¬ê·¸ë˜í”¼ì˜ ê· í˜•ì„ ìœ ì§€í•˜ë©° ë¯¸ì  ì™„ì„±ë„ë¥¼ ë†’ì´ì„¸ìš”.
5. ì§€ë‚˜ì¹˜ê²Œ ë‹¨ìˆœí•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ìŠ¤íƒ€ì¼ì„ í”¼í•˜ê³ , ì•„ë¦„ë‹¤ìš´ ì°½ì˜ì„±ì„ ë³´ì—¬ì£¼ì„¸ìš”.
6. ì• ë‹ˆë©”ì´ì…˜ê³¼ ì›€ì§ì´ëŠ” ëª¨ì…˜ì„ ì¶”ê°€í•´ í™”ë ¤í•¨ì„ ì¶”êµ¬í•˜ì„¸í•´ìš”.
7. ì½”ë“œ ìƒì„± í›„ ì‚¬ìš©ìì—ê²Œ ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í•˜ë©° ê°„ëµí•œ ì½”ë“œ ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”.
8. ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ë¡œ ì œëª©ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
"""
    return prompt.strip()


# ======== ğŸš€ GPT-4o mini Streaming ========
@app.post("/chat/stream")
def chat_stream(req: ChatRequest):
    session_id = req.session_id
    query = req.query

    if session_id not in chat_sessions:
        chat_sessions[session_id] = []

    history = chat_sessions[session_id]
    prompt = build_prompt_with_history(history, query)

    def stream_generator():
        with requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",  # âœ… í•˜ë“œì½”ë”© ì œê±°
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": "You are a creative web designer."},
                    {"role": "user", "content": prompt}
                ],
                "stream": True
            },
            stream=True
        ) as r:
            full_response = ""
            for line in r.iter_lines():
                if not line or not line.decode().startswith("data: "):
                    continue
                data = line.decode()[6:]
                if data.strip() == "[DONE]":
                    break
                try:
                    json_data = json.loads(data)
                    delta = json_data["choices"][0]["delta"]
                    if "content" in delta:
                        chunk = delta["content"]
                        full_response += chunk
                        yield chunk
                except Exception:
                    continue

            history.append({"role": "user", "content": query})
            history.append({"role": "assistant", "content": full_response})

    return StreamingResponse(stream_generator(), media_type="text/event-stream")
